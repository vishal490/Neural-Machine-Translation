{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1fba6dec-f6b2-4c24-95b2-6d7c5278dae3",
      "metadata": {
        "id": "1fba6dec-f6b2-4c24-95b2-6d7c5278dae3"
      },
      "source": [
        "#### I could not save trained model\n",
        "#### So please to check working of this notebook train it on 5 or 10 sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a95d60a-d4f7-4ceb-a860-ad3ccd915aba",
      "metadata": {
        "id": "8a95d60a-d4f7-4ceb-a860-ad3ccd915aba"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "with open('train_data1.json', 'r',encoding=\"utf8\") as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "\n",
        "\n",
        "data_keys = json_data.keys()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c28a72fb-a219-463b-ae7a-75b57944ec9b",
      "metadata": {
        "id": "c28a72fb-a219-463b-ae7a-75b57944ec9b",
        "outputId": "12a4ed16-3ec5-43ec-db2a-7eb8802b7742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Data Type: Train\n",
            "langauages: English-Bengali\n",
            "  Data Type: Test\n",
            "(5, 23)\n",
            "0m 0s (- 0m 2s) (5 6%) 4.0744\n",
            "0m 0s (- 0m 2s) (10 12%) 3.8720\n",
            "0m 0s (- 0m 2s) (15 18%) 3.6622\n",
            "0m 0s (- 0m 2s) (20 25%) 3.4467\n",
            "0m 0s (- 0m 1s) (25 31%) 3.2333\n",
            "0m 1s (- 0m 1s) (30 37%) 3.0330\n",
            "0m 1s (- 0m 1s) (35 43%) 2.8559\n",
            "0m 1s (- 0m 1s) (40 50%) 2.7096\n",
            "0m 1s (- 0m 1s) (45 56%) 2.5930\n",
            "0m 1s (- 0m 0s) (50 62%) 2.4976\n",
            "0m 1s (- 0m 0s) (55 68%) 2.4132\n",
            "0m 1s (- 0m 0s) (60 75%) 2.3363\n",
            "0m 2s (- 0m 0s) (65 81%) 2.2699\n",
            "0m 2s (- 0m 0s) (70 87%) 2.2081\n",
            "0m 2s (- 0m 0s) (75 93%) 2.1501\n",
            "0m 2s (- 0m 0s) (80 100%) 2.0950\n",
            "  Data Type: Train\n",
            "langauages: English-Gujarati\n",
            "  Data Type: Test\n",
            "(5, 26)\n",
            "0m 0s (- 0m 2s) (5 6%) 4.0378\n",
            "0m 0s (- 0m 1s) (10 12%) 3.8719\n",
            "0m 0s (- 0m 1s) (15 18%) 3.7049\n",
            "0m 0s (- 0m 1s) (20 25%) 3.5364\n",
            "0m 0s (- 0m 1s) (25 31%) 3.3669\n",
            "0m 0s (- 0m 1s) (30 37%) 3.2011\n",
            "0m 0s (- 0m 1s) (35 43%) 3.0492\n",
            "0m 1s (- 0m 1s) (40 50%) 2.9206\n",
            "0m 1s (- 0m 0s) (45 56%) 2.8153\n",
            "0m 1s (- 0m 0s) (50 62%) 2.7235\n",
            "0m 1s (- 0m 0s) (55 68%) 2.6336\n",
            "0m 1s (- 0m 0s) (60 75%) 2.5480\n",
            "0m 1s (- 0m 0s) (65 81%) 2.4706\n",
            "0m 1s (- 0m 0s) (70 87%) 2.3992\n",
            "0m 2s (- 0m 0s) (75 93%) 2.3302\n",
            "0m 2s (- 0m 0s) (80 100%) 2.2615\n",
            "  Data Type: Train\n",
            "langauages: English-Hindi\n",
            "  Data Type: Test\n",
            "(5, 40)\n",
            "0m 0s (- 0m 4s) (5 6%) 4.2579\n",
            "0m 0s (- 0m 4s) (10 12%) 4.0343\n",
            "0m 0s (- 0m 3s) (15 18%) 3.8076\n",
            "0m 1s (- 0m 3s) (20 25%) 3.5683\n",
            "0m 1s (- 0m 3s) (25 31%) 3.3099\n",
            "0m 1s (- 0m 2s) (30 37%) 3.0321\n",
            "0m 1s (- 0m 2s) (35 43%) 2.7476\n",
            "0m 2s (- 0m 2s) (40 50%) 2.4837\n",
            "0m 2s (- 0m 1s) (45 56%) 2.2692\n",
            "0m 2s (- 0m 1s) (50 62%) 2.1112\n",
            "0m 3s (- 0m 1s) (55 68%) 1.9957\n",
            "0m 3s (- 0m 1s) (60 75%) 1.9089\n",
            "0m 3s (- 0m 0s) (65 81%) 1.8470\n",
            "0m 3s (- 0m 0s) (70 87%) 1.7980\n",
            "0m 4s (- 0m 0s) (75 93%) 1.7558\n",
            "0m 4s (- 0m 0s) (80 100%) 1.7202\n",
            "  Data Type: Train\n",
            "langauages: English-Kannada\n",
            "  Data Type: Test\n",
            "(5, 15)\n",
            "0m 0s (- 0m 1s) (5 6%) 3.5794\n",
            "0m 0s (- 0m 0s) (10 12%) 3.4945\n",
            "0m 0s (- 0m 0s) (15 18%) 3.4095\n",
            "0m 0s (- 0m 0s) (20 25%) 3.3208\n",
            "0m 0s (- 0m 0s) (25 31%) 3.2293\n",
            "0m 0s (- 0m 0s) (30 37%) 3.1275\n",
            "0m 0s (- 0m 0s) (35 43%) 3.0214\n",
            "0m 0s (- 0m 0s) (40 50%) 2.9131\n",
            "0m 0s (- 0m 0s) (45 56%) 2.8031\n",
            "0m 0s (- 0m 0s) (50 62%) 2.6900\n",
            "0m 0s (- 0m 0s) (55 68%) 2.5771\n",
            "0m 0s (- 0m 0s) (60 75%) 2.4635\n",
            "0m 1s (- 0m 0s) (65 81%) 2.3492\n",
            "0m 1s (- 0m 0s) (70 87%) 2.2352\n",
            "0m 1s (- 0m 0s) (75 93%) 2.1239\n",
            "0m 1s (- 0m 0s) (80 100%) 2.0156\n",
            "  Data Type: Train\n",
            "langauages: English-Malayalam\n",
            "  Data Type: Test\n",
            "(5, 34)\n",
            "0m 0s (- 0m 2s) (5 6%) 4.2602\n",
            "0m 0s (- 0m 1s) (10 12%) 4.1491\n",
            "0m 0s (- 0m 1s) (15 18%) 4.0453\n",
            "0m 0s (- 0m 1s) (20 25%) 3.9436\n",
            "0m 0s (- 0m 1s) (25 31%) 3.8391\n",
            "0m 0s (- 0m 1s) (30 37%) 3.7281\n",
            "0m 1s (- 0m 1s) (35 43%) 3.6061\n",
            "0m 1s (- 0m 1s) (40 50%) 3.4704\n",
            "0m 1s (- 0m 1s) (45 56%) 3.3208\n",
            "0m 1s (- 0m 0s) (50 62%) 3.1626\n",
            "0m 1s (- 0m 0s) (55 68%) 3.0065\n",
            "0m 1s (- 0m 0s) (60 75%) 2.8626\n",
            "0m 1s (- 0m 0s) (65 81%) 2.7318\n",
            "0m 2s (- 0m 0s) (70 87%) 2.6144\n",
            "0m 2s (- 0m 0s) (75 93%) 2.5117\n",
            "0m 2s (- 0m 0s) (80 100%) 2.4197\n",
            "  Data Type: Train\n",
            "langauages: English-Tamil\n",
            "  Data Type: Test\n",
            "(5, 21)\n",
            "0m 0s (- 0m 1s) (5 6%) 3.7581\n",
            "0m 0s (- 0m 1s) (10 12%) 3.5808\n",
            "0m 0s (- 0m 1s) (15 18%) 3.4019\n",
            "0m 0s (- 0m 1s) (20 25%) 3.2221\n",
            "0m 0s (- 0m 1s) (25 31%) 3.0493\n",
            "0m 0s (- 0m 1s) (30 37%) 2.8949\n",
            "0m 0s (- 0m 1s) (35 43%) 2.7673\n",
            "0m 1s (- 0m 1s) (40 50%) 2.6684\n",
            "0m 1s (- 0m 0s) (45 56%) 2.5875\n",
            "0m 1s (- 0m 0s) (50 62%) 2.5145\n",
            "0m 1s (- 0m 0s) (55 68%) 2.4431\n",
            "0m 1s (- 0m 0s) (60 75%) 2.3728\n",
            "0m 1s (- 0m 0s) (65 81%) 2.3064\n",
            "0m 1s (- 0m 0s) (70 87%) 2.2400\n",
            "0m 1s (- 0m 0s) (75 93%) 2.1727\n",
            "0m 2s (- 0m 0s) (80 100%) 2.1064\n",
            "  Data Type: Train\n",
            "langauages: English-Telgu\n",
            "  Data Type: Test\n",
            "(5, 20)\n",
            "0m 0s (- 0m 1s) (5 6%) 4.0448\n",
            "0m 0s (- 0m 1s) (10 12%) 3.9209\n",
            "0m 0s (- 0m 1s) (15 18%) 3.7991\n",
            "0m 0s (- 0m 1s) (20 25%) 3.6720\n",
            "0m 0s (- 0m 1s) (25 31%) 3.5366\n",
            "0m 0s (- 0m 1s) (30 37%) 3.3952\n",
            "0m 0s (- 0m 1s) (35 43%) 3.2558\n",
            "0m 1s (- 0m 1s) (40 50%) 3.1304\n",
            "0m 1s (- 0m 0s) (45 56%) 3.0253\n",
            "0m 1s (- 0m 0s) (50 62%) 2.9325\n",
            "0m 1s (- 0m 0s) (55 68%) 2.8442\n",
            "0m 1s (- 0m 0s) (60 75%) 2.7507\n",
            "0m 1s (- 0m 0s) (65 81%) 2.6612\n",
            "0m 1s (- 0m 0s) (70 87%) 2.5746\n",
            "0m 1s (- 0m 0s) (75 93%) 2.4879\n",
            "0m 2s (- 0m 0s) (80 100%) 2.4027\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for Languages_name in data_keys:\n",
        "\n",
        "            list_of_train_pairs = []\n",
        "\n",
        "            for language_pair, language_data in json_data.items():\n",
        "                if(language_pair == f\"{Languages_name}\"):\n",
        "                  for data_type, data_entries in language_data.items():\n",
        "                      print(f\"  Data Type: {data_type}\")\n",
        "                      print(f\"langauages: {language_pair}\")\n",
        "                      for entry_id, entry_data in data_entries.items():\n",
        "                          source_sentence = entry_data[\"source\"]\n",
        "                          target_sentence = entry_data[\"target\"]\n",
        "                          list_of_train_pairs.append([source_sentence,target_sentence])\n",
        "\n",
        "            # list_of_train_pairs = random.sample(list_of_train_pairs, 5000)\n",
        "\n",
        "            list_of_train_pairs = list_of_train_pairs[0:5]\n",
        "\n",
        "            list_of_train_pairs\n",
        "\n",
        "\n",
        "            with open('test_data1_final.json', 'r',encoding=\"utf8\") as json_file:\n",
        "                json_data_val = json.load(json_file)\n",
        "\n",
        "\n",
        "            entity_id_val = []\n",
        "            list_of_val_source = []\n",
        "\n",
        "            # Process JSON data\n",
        "            for language_pair, language_data in json_data_val.items():\n",
        "                if(language_pair == f\"{Languages_name}\"):\n",
        "                   for data_type, data_entries in language_data.items():\n",
        "                       print(f\"  Data Type: {data_type}\")\n",
        "                       for entry_id, entry_data in data_entries.items():\n",
        "                          source = entry_data[\"source\"]\n",
        "                          list_of_val_source.append([entry_id,source])\n",
        "\n",
        "\n",
        "            list_of_val_source = list_of_val_source[:5]\n",
        "\n",
        "\n",
        "\n",
        "            max_length = 0\n",
        "            max_sentence = \"\"\n",
        "\n",
        "            for i in range(len(list_of_train_pairs)):\n",
        "                current_length = len(list_of_train_pairs[i][0].split(' '))\n",
        "                if current_length > max_length:\n",
        "                    max_length = current_length\n",
        "                    max_sentence = list_of_train_pairs[i][0]\n",
        "\n",
        "\n",
        "            MAX_LENGTH_SOURCE =  max_length + 1\n",
        "\n",
        "\n",
        "            # Initialize variables to keep track of the maximum length and the corresponding sentence\n",
        "            max_length = 0\n",
        "            max_sentence = \"\"\n",
        "\n",
        "            for i in range(len(list_of_train_pairs)):\n",
        "                # Calculate the length of the current sentence\n",
        "                current_length = len(list_of_train_pairs[i][1].split(' '))\n",
        "\n",
        "                # Check if the current sentence is longer than the previous maximum\n",
        "                if current_length > max_length:\n",
        "                    max_length = current_length\n",
        "                    max_sentence = list_of_train_pairs[i][1]\n",
        "\n",
        "            MAX_LENGTH_TGT =  max_length + 1\n",
        "\n",
        "\n",
        "            Start_of_sentence_token = 0\n",
        "            End_of_sentence_token = 1\n",
        "\n",
        "            class Language_formate:\n",
        "                def __init__(self):\n",
        "                    self.word2index = {}\n",
        "                    self.word2count = {}\n",
        "                    self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "                    self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "                def add_sentence(self, sentence):\n",
        "                    for word in sentence.split(' '):\n",
        "                        self.add_word(word)\n",
        "\n",
        "                def add_word(self, word):\n",
        "                    if word not in self.word2index:\n",
        "                        self.word2index[word] = self.n_words\n",
        "                        self.word2count[word] = 1\n",
        "                        self.index2word[self.n_words] = word\n",
        "                        self.n_words += 1\n",
        "                    else:\n",
        "                        self.word2count[word] += 1\n",
        "\n",
        "\n",
        "            input_lang = Language_formate()\n",
        "            output_lang = Language_formate()\n",
        "\n",
        "\n",
        "            for x in list_of_train_pairs:\n",
        "                    input_lang.add_sentence(x[0])\n",
        "                    output_lang.add_sentence(x[1])\n",
        "\n",
        "\n",
        "            # def indexesFromSentence(lang, sentence):\n",
        "            #     return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "            def indexesFromSentence(lang, sentence):\n",
        "                try:\n",
        "                    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "                except KeyError:\n",
        "                    return [0]\n",
        "\n",
        "\n",
        "            def tensorFromSentence(lang, sentence):\n",
        "                indexes = indexesFromSentence(lang, sentence)\n",
        "                indexes.append(End_of_sentence_token)\n",
        "                return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "\n",
        "\n",
        "            def load_data(batch_size,input_lang, output_lang, list_of_train_pairs):\n",
        "                n = len(list_of_train_pairs)\n",
        "\n",
        "\n",
        "                input_ids = np.zeros((n, MAX_LENGTH_SOURCE), dtype=np.int32)\n",
        "                print(input_ids.shape)\n",
        "                target_ids = np.zeros((n, MAX_LENGTH_TGT), dtype=np.int32)\n",
        "\n",
        "                for idx, (inp, tgt) in enumerate(list_of_train_pairs):\n",
        "                    inp_ids = indexesFromSentence(input_lang, inp)\n",
        "                    tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "                    inp_ids.append(End_of_sentence_token)\n",
        "                    tgt_ids.append(End_of_sentence_token)\n",
        "                    input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "                    target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "                train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                                           torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "                train_sampler = RandomSampler(train_data)\n",
        "                train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "                return input_lang, output_lang, train_dataloader\n",
        "\n",
        "            class EncoderLSTM(nn.Module):\n",
        "                def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "                    super(EncoderLSTM, self).__init__()\n",
        "                    self.hidden_size = hidden_size\n",
        "\n",
        "                    self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "                    self.lstm = nn.LSTM(hidden_size, hidden_size,batch_first=True)\n",
        "                    self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "                def forward(self, input):\n",
        "                    embedded = self.dropout(self.embedding(input))\n",
        "                    output, (hidden, cell) = self.lstm(embedded)\n",
        "                    return output, (hidden, cell)\n",
        "\n",
        "            class DecoderGRU(nn.Module):\n",
        "                def __init__(self, hidden_size, output_size):\n",
        "                    super(DecoderGRU, self).__init__()\n",
        "                    self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "                    self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "                    self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "                def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "                    batch_size = encoder_outputs.size(0)\n",
        "                    decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(Start_of_sentence_token)\n",
        "\n",
        "                    # Initialize hx as a tuple with two None elements\n",
        "                    # hx = (None, None)\n",
        "                    hx = encoder_hidden\n",
        "                    # print(hx)\n",
        "\n",
        "                    decoder_outputs = []\n",
        "\n",
        "                    for i in range(MAX_LENGTH_TGT):\n",
        "                        decoder_output, hx = self.forward_step(decoder_input, hx)\n",
        "                        decoder_outputs.append(decoder_output)\n",
        "\n",
        "                        if target_tensor is not None:\n",
        "                                        # Teacher forcing: Feed the target as the next input\n",
        "                                        decoder_input = target_tensor[:, i].unsqueeze(1)  # Teacher forcing\n",
        "                        else:\n",
        "                            # Without teacher forcing: use its own predictions as the next input\n",
        "                            _, topi = decoder_output.topk(1)\n",
        "                            decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "\n",
        "                    decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "                    decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "                    return decoder_outputs, hx, None\n",
        "\n",
        "                def forward_step(self, input, hx):\n",
        "                    output = self.embedding(input)\n",
        "                    output = F.relu(output)\n",
        "\n",
        "                    # Ensure hx is a tuple, even if cell state is not used\n",
        "                    # if not isinstance(hx, tuple):\n",
        "                    #     hx = (hx, None)\n",
        "\n",
        "                    output, hx = self.gru(output, hx)\n",
        "                    # print(hx)\n",
        "                    output = self.out(output)\n",
        "                    return output, hx\n",
        "\n",
        "\n",
        "            def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
        "                      decoder_optimizer, criterion):\n",
        "\n",
        "                total_loss = 0\n",
        "                for data in dataloader:\n",
        "                    input_tensor, target_tensor = data\n",
        "\n",
        "                    encoder_optimizer.zero_grad()\n",
        "                    decoder_optimizer.zero_grad()\n",
        "\n",
        "                    encoder_outputs, (encoder_hidden, cell_state) = encoder(input_tensor)\n",
        "                    decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "                    loss = criterion(\n",
        "                        decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "                        target_tensor.view(-1)\n",
        "                    )\n",
        "                    loss.backward()\n",
        "\n",
        "                    encoder_optimizer.step()\n",
        "                    decoder_optimizer.step()\n",
        "\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "                return total_loss / len(dataloader)\n",
        "\n",
        "            import time\n",
        "            import math\n",
        "\n",
        "            def asMinutes(s):\n",
        "                m = math.floor(s / 60)\n",
        "                s -= m * 60\n",
        "                return '%dm %ds' % (m, s)\n",
        "\n",
        "            def timeSince(since, percent):\n",
        "                now = time.time()\n",
        "                s = now - since\n",
        "                es = s / (percent)\n",
        "                rs = es - s\n",
        "                return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "            def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
        "                           print_every=100):\n",
        "                start = time.time()\n",
        "                print_loss_total = 0  # Reset every print_every\n",
        "\n",
        "                encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "                decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "                criterion = nn.NLLLoss()\n",
        "\n",
        "                for epoch in range(1, n_epochs + 1):\n",
        "                    loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "                    print_loss_total += loss\n",
        "\n",
        "                    if epoch % print_every == 0:\n",
        "                        print_loss_avg = print_loss_total / print_every\n",
        "                        print_loss_total = 0\n",
        "                        print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                                    epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "                with torch.no_grad():\n",
        "                    input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "\n",
        "                    encoder_outputs, (encoder_hidden,hidden_cell) = encoder(input_tensor)\n",
        "                    decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "                    _, topi = decoder_outputs.topk(1)\n",
        "                    decoded_ids = topi.squeeze()\n",
        "                    # print(decoded_ids)\n",
        "\n",
        "                    decoded_words = []\n",
        "                    for idx in decoded_ids:\n",
        "                        if idx.item() == End_of_sentence_token:\n",
        "                            decoded_words.append('<EOS>')\n",
        "                            break\n",
        "                        decoded_words.append(output_lang.index2word[idx.item()])\n",
        "                    # print(decoded_words)\n",
        "                return decoded_words\n",
        "\n",
        "            def evaluateRandomly(encoder, decoder, n=10):\n",
        "                data_write = []\n",
        "                for i in range(len(list_of_val_source)):\n",
        "                    pair = list_of_val_source\n",
        "                    output_words = evaluate(encoder, decoder, pair[i][1], input_lang, output_lang)\n",
        "                    result = \" \"\n",
        "                    result_sen = result.join(output_words)\n",
        "                    output_sentence = pair[i][0]+\" \"+'\"'+result_sen+'\"'\n",
        "                    result_string =output_sentence.replace(\"<EOS>\", \"\" )\n",
        "                    data_write.append(result_string)\n",
        "\n",
        "                import csv\n",
        "\n",
        "                # Create or open a CSV file in write mode\n",
        "                csv_file_name = f\"{Languages_name}.csv\"\n",
        "                with open(csv_file_name, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "                   # for x in data_write:\n",
        "                   #     csv_writer = csv.writer(csv_file)\n",
        "                   #     csv_writer.writerow([x])\n",
        "                    csv_writer = csv.writer(csv_file)\n",
        "                    for sentence in data_write:\n",
        "                            # Ensure there are no extra spaces by stripping the sentence\n",
        "                            cleaned_sentence = sentence.strip()\n",
        "                            # Split the sentence into words and write them as a list\n",
        "                            word_list = cleaned_sentence.split()\n",
        "                            csv_file.write(cleaned_sentence + '\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            hidden_size = 32\n",
        "            batch_size = 32\n",
        "\n",
        "            input_lang, output_lang, train_dataloader = load_data(batch_size,input_lang,output_lang,list_of_train_pairs)\n",
        "\n",
        "\n",
        "            encoder = EncoderLSTM(input_lang.n_words, hidden_size).to(device)\n",
        "            decoder = DecoderGRU(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "            train(train_dataloader, encoder, decoder, 80, print_every=5)\n",
        "\n",
        "            # import pickle\n",
        "            # with open(f\"{Languages_name}_enc\",'wb') as f:\n",
        "            #     pickle.dump(encoder,f)\n",
        "\n",
        "            # with open(f\"{Languages_name}_dec\",'wb') as f:\n",
        "            #      pickle.dump(decoder,f)\n",
        "\n",
        "\n",
        "            encoder.eval()\n",
        "            decoder.eval()\n",
        "            evaluateRandomly(encoder, decoder, list_of_val_source)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2363e9b5-790f-4c61-884f-c4a59529e6b5",
      "metadata": {
        "id": "2363e9b5-790f-4c61-884f-c4a59529e6b5",
        "outputId": "039b4e65-4aff-425f-c942-3edbb71f55a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English-Bengali.csv   5\n",
            "English-Gujarati.csv   5\n",
            "English-Hindi.csv   5\n",
            "English-Kannada.csv   5\n",
            "English-Malayalam.csv   5\n",
            "English-Tamil.csv   5\n",
            "English-Telgu.csv   5\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "records = []\n",
        "for file in os.listdir(\".\"):\n",
        "    # print(file)\n",
        "\n",
        "    if file.split(\".\")[0] == \"answer\":\n",
        "        continue\n",
        "\n",
        "    if file.split(\".\")[-1] == \"csv\":\n",
        "        with open(f\"./{file}\", encoding='utf-8') as f:\n",
        "            record = f.readlines()\n",
        "            print(f\"{file}   {len(record)}\")\n",
        "            records.extend(record)\n",
        "            # records.extend(f.readlines())\n",
        "\n",
        "# records[:100]\n",
        "\n",
        "len(records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd12428-00d5-47a5-a124-fd106b5508c6",
      "metadata": {
        "id": "ddd12428-00d5-47a5-a124-fd106b5508c6",
        "outputId": "4268fbe1-67c8-438f-9062-d7684351e322"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "anser.csv have  35 Entries\n"
          ]
        }
      ],
      "source": [
        "with open(\"./answer.csv\", \"w+\", encoding=\"utf-8\") as file:\n",
        "    # for ele in records:\n",
        "    file.writelines(records)\n",
        "print(\"anser.csv have \",len(records),\"Entries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06bd4c6a-56a1-467f-9bc8-5ea92b09f490",
      "metadata": {
        "id": "06bd4c6a-56a1-467f-9bc8-5ea92b09f490"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2557d56e-a22d-4210-8002-e85c10105b0c",
      "metadata": {
        "id": "2557d56e-a22d-4210-8002-e85c10105b0c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}